\documentclass[letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{mathrsfs}

\usepackage{afterpage}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{verse}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\usepackage{epstopdf}
\usepackage{circuitikz}
\usepackage[separate-uncertainty = true,multi-part-units=single]{siunitx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage[toc,page]{appendix}
\usepackage{color}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{multirow}
\usepackage{makecell}
\usepackage[round]{natbib}   % omit 'round' option if you prefer square brackets
\usepackage{titling}
\usepackage{siunitx}
\usepackage{physics}

\usepackage{setspace}
% \doublespacing
\usepackage{float}


\pgfplotsset{compat=1.14}

%  Special math symbols
%       floor, ceiling, angled brackets
%-----------------------------------------------------------------------
\newcommand{\floor}[1]{\left\lfloor #1\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1\right\rceil}
\newcommand{\etal}{\textit{et al.}}
\newcommand{\RE}{\mathbb{R}}        % real space
\newcommand{\ZZ}{\mathbb{Z}}        % integers
\newcommand{\NN}{\mathbb{N}}        % natural numbers
\newcommand{\eps}{{\varepsilon}}    % prettier epsilon
%-----------------------------------------------------------------------
%  Tighter lists
%-----------------------------------------------------------------------
\newenvironment{itemize*}% Tighter itemized list
  {\begin{itemize}%
    \setlength{\itemsep}{-0.5ex}%
    \setlength{\parsep}{0pt}}%
  {\end{itemize}}
\newenvironment{description*}% Tighter description list
  {\begin{description}%
    \setlength{\itemsep}{-0.5ex}%
    \setlength{\parsep}{0pt}}%
  {\end{description}}
\newenvironment{enumerate*}% Tighter enumerated list
  {\begin{enumerate}%
    \setlength{\itemsep}{-0.5ex}%
    \setlength{\parsep}{0pt}}%
  {\end{enumerate}}
%-----------------------------------------------------------------------
% Typing shortcuts
%-----------------------------------------------------------------------
\newcommand{\X}{\mathbb{X}}
\newcommand{\SG}{\mathbf{S}}
\newcommand{\GE}{\mathcal{G}}
\newcommand{\ST}{\,:\,}
\renewcommand{\tilde}[1]{\widetilde{#1}}
\newcommand{\diam}{\mathrm{diam}}
\newcommand{\sq}{\square}
\newcommand{\half}[1]{\frac{#1}{2}}
\newcommand{\inv}[1]{\frac{1}{#1}}
\newcommand{\alg}{\textsf{SplitReduce}}
\newcommand{\sz}[1]{\sigma_{#1}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\softOmega}{\widetilde{\Omega}} 
\newcommand{\softO}{\widetilde{O}}
\newcommand{\OO}{O^*}  %or \widetilde{O}?

\newcommand{\dx}{\mathrm{d}x}
\newcommand{\dy}{\mathrm{d}y}
\newcommand{\dz}{\mathrm{d}z}
\newcommand{\dt}{\mathrm{d}t}
\newcommand{\du}{\mathrm{d}u}
\newcommand{\dtheta}{\mathrm{d}\theta}
\newcommand{\dq}{\mathrm{d}q}
\newcommand{\diff}{\mathrm{d}}
\newcommand{\dV}{\mathrm{d}V}
\newcommand{\dL}{\mathrm{d}L}
\newcommand{\dA}{\mathrm{d}A}
\newcommand{\dH}{\mathrm{d}H}
\newcommand{\df}{\mathrm{d}f}
\newcommand{\dg}{\mathrm{d}g}
\newcommand{\dr}{\mathrm{d}r}
\newcommand{\dw}{\mathrm{d}w}
\newcommand{\dI}{\mathrm{d}I}

\newcommand*\len[1]{\overline{#1}}


\newcommand\note[1]{\marginpar{\textcolor{red}{#1}}}
\newcommand*{\tageq}{\refstepcounter{equation}\tag{\theequation}}

\newcommand*{\equals}{=}

\usepackage{fancyhdr}

\pgfplotscreateplotcyclelist{grayscale}{
    thick,white!10!black,mark=x,mark options=solid, dashed\\%
    thick,white!20!black,mark=o,mark options=solid\\%
}

\newcommand{\mat}[1]{\ensuremath{\begin{bmatrix}#1\end{bmatrix}}}
\newcommand{\cat}[1]{\ensuremath{\begin{vmatrix}#1\end{vmatrix}}}
\newcommand{\eqn}[1]{\begin{alignat*}{2}#1\end{alignat*}}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}
\newcommand*{\thus}{&\implies\quad&}

\newcommand{\answer}[1]{\framebox{$\displaystyle #1 $}}

\newcommand{\shrug}[1][]{%
\begin{tikzpicture}[baseline,x=0.8\ht\strutbox,y=0.8\ht\strutbox,line width=0.125ex,#1]
\def\arm{(-2.5,0.95) to (-2,0.95) (-1.9,1) to (-1.5,0) (-1.35,0) to (-0.8,0)};
\draw \arm;
\draw[xscale=-1] \arm;
\def\headpart{(0.6,0) arc[start angle=-40, end angle=40,x radius=0.6,y radius=0.8]};
\draw \headpart;
\draw[xscale=-1] \headpart;
\def\eye{(-0.075,0.15) .. controls (0.02,0) .. (0.075,-0.15)};
\draw[shift={(-0.3,0.8)}] \eye;
\draw[shift={(0,0.85)}] \eye;
% draw mouth
\draw (-0.1,0.2) to [out=15,in=-100] (0.4,0.95); 
\end{tikzpicture}}


\pagestyle{fancy}
\fancyhf{}
\rhead{Rahul Arya}
\lhead{EE 16B}
\cfoot{\thepage}

\title{Lecture 12 - Notes}
\author{Rahul Arya}
\date{February 2019}
\begin{document}

\maketitle

\section{Overview}
From the past few lectures, we now know, given a discrete-time linear system, how to apply inputs to drive it towards a desired state, even when our inputs were transformed by a matrix $B$. However, in doing so, we relied on a complete knowledge of the initial state $\vec{x}[0]$. Now, we will consider the dual problem of \emph{observability} - given limited observations of the system's evolution over time, can we determine its initial internal state $\vec{x}[0]$?

\section{The Observability Problem}
First, we will state the problem more explicitly. Consider the following discrete system:
\eqn{
    && \vec{x}[i + 1] &= A\vec{x}[i] + B\vec{u}[i] \\
    && \vec{y}[i] &= C\vec{x}[i].
}
The system will evolve over time due to a series of control inputs $\vec{u}[i]$ that we apply, in the following manner:
\eqn{
    && \vec{x}[0] &= \vec{x}[0] \\
    && \vec{x}[1] &= A\vec{x}[0] + B\vec{u}[0] \\
    && \vec{x}[2] &= A^2\vec{x}[0] + AB\vec{u}[0] + B\vec{u}[1] \\
    && \vdots &
}
Clearly, if we could see the states $\vec{x}[0], \vec{x}[1], \vec{x}[2], \ldots$ it would be trivial to recover $\vec{x}[0]$ - after all, we are told it explicitly! However, in physical systems it is very rare for us to be able to view the entirety of the state at any given time. Recall that we can use these discrete time models to approximate the behavior of an analog circuit with varying input. In such a scenario, it would be very unlikely for us to be able to measure \emph{every} current, voltage, and charge throughout the system.

Instead, we may only be able to observe a linearly transformed version of the state, which we denote as $\vec{y}$. Thus, the question becomes - given the sequence of observations
\[
    (\vec{y}[0], \vec{u}[0]), (\vec{y}[1], \vec{u}[1]), (\vec{y}[2], \vec{u}[2]), \ldots,
\]
can we recover the initial state $\vec{x}[0]$?

\section{Simplifying the Problem}
To do so, we will first consider a simpler problem, when no input is present in the system. In other words, the system can be thought of as:
\eqn{
    && \vec{x}[i + 1] &= A\vec{x}[i] \\
    && \vec{y}[i] &= C\vec{x}[i].
}
This simplification is convenient since it means that we no longer have to worry about choosing our inputs. From this, we can immediately express
\[
    \vec{y}[0] = C\vec{x}[0].
\]
If $C$ were invertible, then it would be trivial to recover $\vec{x}[0]$ without even letting the system evolve. However, typically $C$ is a ``wide'' matrix, taking in a higher dimensional state vector and returning a low dimensional vector of observations, so that tends not to be the case. We use $n$ to denote the dimension of our state vector $\vec{x}$, and $k$ to denote the dimension of the observation vector $\vec{y}$, so $C$ has $k$ rows and $n$ columns.

However, we saw last time when we looked at observability that allowing a system more time to evolve gave us more control over its behavior. It is natural to conjecture that allowing our system more time to evolve will similarly allow us to obtain more information about its state. Indeed, after $i$ time steps, we will obtain the equations
\eqn{
    && \vec{y}[0] &= C\vec{x}[0] \\
    && \vec{y}[1] &= CA\vec{x}[0] \\
    && \vec{y}[2] &= CA^2\vec{x}[0] \\
    && \vdots \\
    && \vec{y}[i] &= CA^i\vec{x}[0].
}
Rewriting them in stacked matrix form, we obtain the system
\[
    \mat{C \\ CA \\ CA^2 \\ \vdots \\ CA^i} \vec{x}[0] = \mat{\vec{y}[0] \\ \vec{y}[1] \\ \vec{y}[2] \\ \vdots \\ \vec{y}[i]}.
\]
We typically refer to the matrix on the left as the observability matrix, denoted as $\mathscr{O}$. Therefore, we can use least squares to recover $\vec{x}[0]$, to obtain
\[
    \vec{x}[0] = (\mathscr{O}^T\mathscr{O})^{-1}\mathscr{O}^T \mat{\vec{y}[0] \\ \vec{y}[1] \\ \vec{y}[2] \\ \vdots \\ \vec{y}[i]}.
\]
From EE16A, we know that this approach works exactly when $\mathscr{O}$ does not have a nontrivial nullspace (i.e. has linearly independent columns).

\section{Observability with Inputs}
Now, we will return to the general case, when we can supply inputs $\vec{u}[i]$. Clearly,
\eqn{
    && \vec{y}[0] &= C\vec{x}[0] \\
    && \vec{y}[1] &= CA\vec{x}[0] + CB\vec{u}[0] \\
    && \vec{y}[2] &= CA^2\vec{x}[0] + CAB\vec{u}[0] + CB\vec{u}[1] \\
    && \vdots \\
    && \vec{y}[i] &= CA^i\vec{x}[0] + CA^{i-1}B\vec{u}[0] + CA^{i - 2}B\vec{u}[1] + \ldots + CB\vec{u}[i-1].
}
In addition, let us define $\vec{y}_{free}$ to be the observations that we would have gotten had our inputs all been $0$. From the previous section, we know that
\eqn{
    && \vec{y}_{free}[0] &= C\vec{x}[0] \\
    && \vec{y}_{free}[1] &= CA\vec{x}[0] \\
    && \vec{y}_{free}[2] &= CA^2\vec{x}[0] \\
    && \vdots \\
    && \vec{y}_{free}[i] &= CA^i\vec{x}[0].
}

Observe that we may rewrite any given $\vec{y}[j]$ as follows
\eqn{
    && \vec{y}[j] &= CA^j\vec{x}[0] + CA^{j-1}B\vec{u}[0] + CA^{j-2}B\vec{u}[1] + \ldots + CB\vec{u}[j-1] \\
    &&&= CA^j\vec{x}[0] + \sum_{k=0}^{j-1} CA^{j - k - 1}\vec{u}[k] \\
    &&&= \vec{y}_{free}[j] + \sum_{k=0}^{j-1} CA^{j - k - 1}\vec{u}[k].
}

Critically, notice that the second term in the sum for $\vec{y}[j]$ has no dependence on the initial state, and so can be determined exactly given only the system and the applied inputs. Therefore, it gives us neither more or less information about the initial state, no matter what inputs we apply. As a result, we can focus on the case when $\vec{u} = 0$, since we will always be able to recover $\vec{y}_{free}$ from $\vec{y}$ by subtracting out the known dependence on the inputs.

Recall from last lecture that a system is controllable in infinite timesteps if and only if it is controllable in at most $n$ timesteps. The exact same proof applies here in the context of observability, and so will not be repeated. That is to say, we can determine the initial state $\vec{x}[0]$ after an arbitrary many number of observations made each timestep if and only if we can do so in at most $n$ timesteps. Therefore, we typically use $\mathscr{O}$ to refer to the observations over $n$ timesteps, so we have
\[
    \mathscr{O} = \mat{C \\ CA \\ CA^2 \\ \vdots \\ CA^{n - 1}}.
\]

Consequently, when attempting to determine the initial state of a system, we should apply arbitrary inputs (or have them applied for us, if we cannot control the inputs), observe the first $n$ values of $\vec{y}[i]$ (from $\vec{y}[0]$ through $\vec{y}[n - 1]$), use our knowledge of the inputs to deduce the corresponding $\vec{y}_{free}$, and solve the least squares system for $\vec{x}[0]$ (or report that no unique solution exists).

\section{Some Examples}
To complete our discussion of observability, we will look at some toy examples. Consider
\eqn{
    && A &= \mat{1 & -1 \\ 2 & 1} \\
    && C &= \mat{1 & 0}.
}

In other words, we can only observe the first component of our state vector at any given time. Notice that since our inputs are of no importance is determining the observability of a system, we do not need to know $B$.

We see that the observability matrix is
\eqn{
    && \mathscr{O} &= \mat{1 & 0 \\ 1 & -1}.
}
Clearly, $\mathscr{O}$ is of full rank, so we should be able to recover the initial state $\vec{x}[0]$ from the first $n = 2$ observations. Specifically, if $\vec{x}[0] = \mat{a \\ b}$, we obtain
\eqn{
    && \vec{x}[1] &= \mat{a - b \\ 2a - b} \\
    \thus \vec{y}[0] &= \mat{a} \\
    && \vec{y}[1] &= \mat{a - b}.
}
Clearly, given $a$ and $a - b$, we can recover both components of the original state vector exactly.

However, now consider the alternative system
\eqn{
    && A &= \mat{2 & 0 \\ 2 & 3} \\
    && C &= \mat{1 & 0}.
}
We see that the observability matrix is
\eqn{
    && \mathscr{O} &= \mat{1 & 0 \\ 2 & 0},
}
so we will not be able to recover the initial state.


\section{System Identification}


\end{document}
