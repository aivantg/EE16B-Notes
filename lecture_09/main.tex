\documentclass[letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}

\usepackage{afterpage}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{verse}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\usepackage{epstopdf}
\usepackage{circuitikz}
\usepackage[separate-uncertainty = true,multi-part-units=single]{siunitx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage[toc,page]{appendix}
\usepackage{color}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\usepackage{multirow}
\usepackage{makecell}
\usepackage[round]{natbib}   % omit 'round' option if you prefer square brackets
\usepackage{titling}
\usepackage{siunitx}
\usepackage{physics}

\usepackage{setspace}
% \doublespacing
\usepackage{float}


\pgfplotsset{compat=1.14}

%  Special math symbols
%       floor, ceiling, angled brackets
%-----------------------------------------------------------------------
\newcommand{\floor}[1]{\left\lfloor #1\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1\right\rceil}
\newcommand{\etal}{\textit{et al.}}
\newcommand{\RE}{\mathbb{R}}        % real space
\newcommand{\ZZ}{\mathbb{Z}}        % integers
\newcommand{\NN}{\mathbb{N}}        % natural numbers
\newcommand{\eps}{{\varepsilon}}    % prettier epsilon
%-----------------------------------------------------------------------
%  Tighter lists
%-----------------------------------------------------------------------
\newenvironment{itemize*}% Tighter itemized list
  {\begin{itemize}%
    \setlength{\itemsep}{-0.5ex}%
    \setlength{\parsep}{0pt}}%
  {\end{itemize}}
\newenvironment{description*}% Tighter description list
  {\begin{description}%
    \setlength{\itemsep}{-0.5ex}%
    \setlength{\parsep}{0pt}}%
  {\end{description}}
\newenvironment{enumerate*}% Tighter enumerated list
  {\begin{enumerate}%
    \setlength{\itemsep}{-0.5ex}%
    \setlength{\parsep}{0pt}}%
  {\end{enumerate}}
%-----------------------------------------------------------------------
% Typing shortcuts
%-----------------------------------------------------------------------
\newcommand{\X}{\mathbb{X}}
\newcommand{\SG}{\mathbf{S}}
\newcommand{\GE}{\mathcal{G}}
\newcommand{\ST}{\,:\,}
\renewcommand{\tilde}[1]{\widetilde{#1}}
\newcommand{\diam}{\mathrm{diam}}
\newcommand{\sq}{\square}
\newcommand{\half}[1]{\frac{#1}{2}}
\newcommand{\inv}[1]{\frac{1}{#1}}
\newcommand{\alg}{\textsf{SplitReduce}}
\newcommand{\sz}[1]{\sigma_{#1}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\softOmega}{\widetilde{\Omega}} 
\newcommand{\softO}{\widetilde{O}}
\newcommand{\OO}{O^*}  %or \widetilde{O}?

\newcommand{\dx}{\mathrm{d}x}
\newcommand{\dy}{\mathrm{d}y}
\newcommand{\dz}{\mathrm{d}z}
\newcommand{\dt}{\mathrm{d}t}
\newcommand{\du}{\mathrm{d}u}
\newcommand{\dtheta}{\mathrm{d}\theta}
\newcommand{\dq}{\mathrm{d}q}
\newcommand{\diff}{\mathrm{d}}
\newcommand{\dV}{\mathrm{d}V}
\newcommand{\dL}{\mathrm{d}L}
\newcommand{\dA}{\mathrm{d}A}
\newcommand{\dH}{\mathrm{d}H}
\newcommand{\df}{\mathrm{d}f}
\newcommand{\dg}{\mathrm{d}g}
\newcommand{\dr}{\mathrm{d}r}
\newcommand{\dw}{\mathrm{d}w}
\newcommand{\dI}{\mathrm{d}I}

\newcommand*\len[1]{\overline{#1}}


\newcommand\note[1]{\marginpar{\textcolor{red}{#1}}}
\newcommand*{\tageq}{\refstepcounter{equation}\tag{\theequation}}

\newcommand*{\equals}{=}

\usepackage{fancyhdr}

\pgfplotscreateplotcyclelist{grayscale}{
    thick,white!10!black,mark=x,mark options=solid, dashed\\%
    thick,white!20!black,mark=o,mark options=solid\\%
}

\newcommand{\mat}[1]{\ensuremath{\begin{bmatrix}#1\end{bmatrix}}}
\newcommand{\cat}[1]{\ensuremath{\begin{vmatrix}#1\end{vmatrix}}}
\newcommand{\eqn}[1]{\begin{alignat*}{2}#1\end{alignat*}}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}
\newcommand*{\thus}{&\implies\quad&}

\newcommand{\answer}[1]{\framebox{$\displaystyle #1 $}}

\newcommand{\shrug}[1][]{%
\begin{tikzpicture}[baseline,x=0.8\ht\strutbox,y=0.8\ht\strutbox,line width=0.125ex,#1]
\def\arm{(-2.5,0.95) to (-2,0.95) (-1.9,1) to (-1.5,0) (-1.35,0) to (-0.8,0)};
\draw \arm;
\draw[xscale=-1] \arm;
\def\headpart{(0.6,0) arc[start angle=-40, end angle=40,x radius=0.6,y radius=0.8]};
\draw \headpart;
\draw[xscale=-1] \headpart;
\def\eye{(-0.075,0.15) .. controls (0.02,0) .. (0.075,-0.15)};
\draw[shift={(-0.3,0.8)}] \eye;
\draw[shift={(0,0.85)}] \eye;
% draw mouth
\draw (-0.1,0.2) to [out=15,in=-100] (0.4,0.95); 
\end{tikzpicture}}


\pagestyle{fancy}
\fancyhf{}
\rhead{Rahul Arya}
\lhead{EE 16B}
\cfoot{\thepage}

\title{Lecture 9 - Notes}
\author{Rahul Arya}
\date{February 2019}
\begin{document}

\maketitle

\section{Overview}
In the past few lectures, we have explored the technique of phasor analysis in the frequency domain, calculating the ``steady-state'' of circuits driven by sinusoidal current sources. In doing so, we have neglected the initial transient behavior of these circuits. For instance, imagine a sequence of chained RC-filters where the capacitors have some initial charge, as shown:
\begin{center}
\begin{circuitikz}[american]
\draw (0, 0) node[ocirc]{} node[left]{$V_{in}$} to[R, l=$R_1$] (2, 0) to[C, l=$C_1$] (2, -2) node[ground] {};
\draw (2, 0) to[R, l=$R_2$] (4, 0) to[C, l=$C_2$] (4, -2) node[ground] {};
\end{circuitikz}
\end{center}
Intuitively, we can see that any initial charge on the capacitors will discharge through the resistors (just as if $V_{in}$ were instead grounded), with the circuit quickly converging to the sinusoidal, steady-state behavior. However, we currently have no systematic method for calculating the initial behavior (known as the \emph{transient state}, or the \emph{warmup-period}), especially for more complex states.

We will develop such a method in this lecture.

\section{Homogenous Transients}
First, we will deal with a slightly easier question, by assuming that our circuit's eventual steady state is the ``zero-state'' (i.e. no nonzero currents or voltages). Clearly, for our circuit to have that property, it cannot be driven by any external voltage sources, since then the steady-state would have at least one nonzero voltage. Thus, we will consider a simplified version of the above solution with $V_{in}$ grounded, as shown:
\begin{center}
\begin{circuitikz}[american]
\draw (0, 0) node[ground]{} to[R, l=$R_1$] (2, 0) to[C, l=$C_1$] (2, -2) node[ground] {};
\draw (2, 0) to[R, l=$R_2$] (4, 0) to[C, l=$C_2$] (4, -2) node[ground] {};
\end{circuitikz}
\end{center}
To establish our initial conditions, assume that $V_{in}$ was initially \SI{1}{\volt} for a long period of time (so the system reached an equilibrium / steady-state), and was suddenly grounded at some time $t = 0$. 

Now, in order to ``solve'' any system, we need to establish what exactly we are solving for. Let's first very mechanically label all the circuit quantities that could possibly be of interest to us, as shown:
\begin{center}
\begin{circuitikz}[american]
\draw (0, 0) node[ground]{} to[R, l=$R_1$, i=$I_{R_1}$] (2, 0) to[C, l=$C_1$, i=$I_{C_1}$] (2, -2) node[ground] {};
\draw (2, 0) to[R, l=$R_2$, i=$I_{R_2}$] (4, 0) node[ocirc] {} node[right] {$V_2$} to[C, l=$C_2$, i=$I_{C_2}$] (4, -2) node[ground] {};
\draw (2, 0) node[ocirc] {} node[below right] {$V_1$};
\end{circuitikz}
\end{center}
By Ohm's law, the capacitor equation, and KCL, we obtain the following equations
\eqn{
    && I_{R_1}R_1 &= 0 - V_1 \\
    && I_{R_2}R_2 &= V_1 - V_2 \\
    && I_{C_1} / C_1 &= \frac{\diff V_1}{\dt} \\
    && I_{C_2} / C_2 &= \frac{\diff V_2}{\dt} \\
    && I_{R_1} &= I_{C_1} + I_{R_2} \\
    && I_{R_2} &= I_{C_2}.
}
So far, what we've been doing is very similar to the procedures we saw in EE16A, as well as in phasor analysis. And so far, it seems to be working. We've got $6$ unknowns and $6$ equations, so, at least in theory, we should be able to get a solution!

The issue, of course, is that these equations are not simply linear equations in the form we are used to, but rather are \emph{linear differential equations}, so our old techniques to Gaussian elimination break down.

In fact, from the perspective of Gaussian elimination, the derivative terms $\frac{\diff V_1}{\dt}$ and $\frac{\diff V_2}{\dt}$ are effectively completely different variables! If we knew their values, we could easily solve for all the other components. In a similar manner, if we were told the values of $V_1$ and $V_2$, we could solve for all the other variables (including $\frac{\diff V_1}{\dt}$ and $\frac{\diff V_2}{\dt}$).

With this in mind, let's try solving for the derivative terms in terms of $V_1$ and $V_2$, which we know is possible by manipulating these equations as if they were purely linear equations:
\eqn{
    && I_{R_1}R_1 &= 0 - V_1 \\
    && I_{R_2}R_2 &= V_1 - V_2 \\
    && I_{C_1} / C_1 &= \frac{\diff V_1}{\dt} \\
    && I_{C_2} / C_2 &= \frac{\diff V_2}{\dt} \\
    && I_{R_1} &= I_{C_1} + I_{R_2} \\
    && I_{R_2} &= I_{C_2} \\
    \thus I_{R_1} &= -\frac{V_1}{R_1} \\
    && I_{R_2} &= \frac{V_1 - V_2}{R_2} \\
    \thus I_{C_1} &= I_{R_1} - I_{R_2} \\
    &&&= -\frac{V_1}{R_1}+ \frac{V_2 - V_1}{R_2} \\
    && I_{C_2} &= \frac{V_1 - V_2}{R_2} \\
    \thus \frac{\diff V_1}{\dt} &= -\left(\frac{1}{R_1C_1} + \frac{1}{R_2C_1}\right) V_1 + \frac{1}{R_2C_1}V_2 \\
    && \frac{\diff V_2}{\dt} &= \frac{1}{R_2C_2}V_1 - \frac{1}{R_2C_2}V_2.
}
Now, all we need to do is compute either $V_1$ and $V_2$, or $\frac{\diff V_1}{\dt}$ and $\frac{\diff V_2}{\dt}$ - after that, we can use our linear system to solve for the complete behavior of the circuit. Because of this, we call $V_1$ and $V_2$ our \emph{state variables}, and call the above two differential equations the \emph{state-space representation} of our system, since solving them is equivalent to solving for the entire behavior of our circuit.

\section{Matrix Differential Equations}
Unfortunately, we have at this point obtained two \emph{coupled} differential equations, where $\frac{\diff V_1}{\dt}$ depends on $V_2$ (and vice-versa), so we cannot solve them using known techniques.

The key insight here is to convert our coupled differential equations, dealing with scalar quantities varying over time, into a single equation involving a vector varying over time. We can do so by constructing the following system
\[
    \frac{\diff}{\dt} \mat{V_1 \\ V_2} = \mat{-\frac{1}{R_1C_1} - \frac{1}{R_2C_1} & \frac{1}{R_2C_1} \\ \frac{1}{R_2C_2} & -\frac{1}{R_2C_2}} \mat{V_1 \\ V_2}.
\]
This equation is exactly the same thing as what we had previously, just written using a matrix. Notice that taking the derivative of a vector is equivalent to taking the derivative of each of its components - this should be clear either intuitively, or from the definition of the derivative.

Typically, we write this equation in the form
\[
    \frac{\diff \vec{x}}{\dt} = A\vec{x},
\]
where $\vec{x} = \mat{V_1 \\ V_2}$ is known as the \emph{state vector}, and $A$ is the large matrix we had previously, known as the \emph{state matrix}\footnote{Unoriginal naming conventions strike again!}.

At first, this does not seem very useful - we've just rewritten our system in a different but algebraically equivalent form, what's the big deal?

To answer this, recall that while we do not yet know how to solve coupled differential equations, we know what to do with single-variable differential equations. Given the equation
\[
    \frac{\diff x}{\dt} &= \lambda x,
\]
we know that
\[
    x(t) = x_0 e^{\lambda t},
\]
for a constant $x_0$ that depends on the initial state, is the unique solution.

Now, consider a set of differential equations in the form
\eqn{
    && \frac{\diff x_1}{\dt} &= \lambda_1 x_1 \\
    && \frac{\diff x_2}{\dt} &= \lambda_2 x_2 \\
    && \vdots \\
    && \frac{\diff x_n}{\dt} &= \lambda_n x_n. 
}
Obviously, each of the $x_i$ are independent from the rest, so we may solve this system (given initial conditions) by simply solving each equation in turn.

The key insight here is that the above, simplified, set of differential equations can be represented in matrix-vector form as
\[
    \frac{\diff\vec{x}}{\dt}  = \Lambda \vec{x},
\]
where $\vec{x} = \mat{x_1 & x_2 & \cdots & x_n}^T$, and $\Lambda$ is a square diagonal matrix whose entries are the $\lambda_i$.

So if we could simply somehow convert our initial state matrix $A$ into a diagonal matrix $\Lambda$, we'd essentially be done. Of course, we know how to do this! For a diagonalizable matrix $A$\footnote{The homework will deal with some of the cases when $A$ is not diagonalizable.}, we may write it as
\[
    A = \mat{| & & | \\ \vec{v_1} & \cdots & \vec{v_n} \\ | & & |} \Lambda \mat{| & & | \\ \vec{v_1} & \cdots & \vec{v_n} \\ | & & |}^{-1} = V\Lambda V^{-1},
\]
where $V$ is the eigenvector matrix of $A$. Substituting into our original state equation and rearranging, we obtain the system
\eqn{
    && \frac{\diff \vec{x}}{\dt} &= V\Lambda V^{-1}\vec{x} \\
    \thus \frac{\diff}{\dt} (V^{-1} \vec{x}) &= \Lambda (V^{-1} \vec{x}).
}

We now have a diagonal matrix relating $V^{-1}\vec{x}$ with its time derivative.For convenience, we typically define $\tilde{x} = V^{-1}\vec{x}$. We know how to solve for $\tilde{x}$ as a function of time, so we can solve for $\vec{x} = V\tilde{x}$.

It is critical to notice that all the operations we perform are reversible, so any solution for $\vec{x}$ will correspond to a solution for $\tilde{x}$, and vice-versa. We can use this fact to prove a uniqueness theorem for our original system. Imagine that we had two different solutions for $\vec{x}$. As our operations are reversible, they would correspond to two different solutions for $\tilde{x}$. But we have previously shown that our solution for $\tilde{x}$ is unique, so we have reached a contradiction. Therefore, we know that the solution to $\vec{x}$ will be unique.

\section{Example}
Let's see how this works in a real example, by returning to our circuits example from earlier. We will select the circuit component values
\eqn{
    && R_1 &= \SI{1/3}{\ohm} \\
    && R_2 &= \SI{1/2}{\ohm} \\
    && C_1 &= \SI{1}{\farad} \\
    && C_2 &= \SI{1}{\farad}.
}
We select these values purely for numerical convenience - you are very unlikely to encounter a $\SI{1}{\farad}$ capacitor in lab!

Thus, substituting into our earlier equations, we obtain the state matrix
\[
    A = \mat{-5 & 2 \\ 2 & -2}.
\]
Solving for the eigenvalues in the standard manner,
\eqn{
    && \abs{A - \lambda I} &= 0 \\
    \thus \cat{-5 - \lambda & 2 \\ 2 & -2 - \lambda} &= 0 \\
    \thus (-5-\lambda)(-2-\lambda) - 4 &= 0 \\
    \thus \lambda^2 + 7\lambda + 6 &= 0 \\
    \thus (\lambda + 1)(\lambda + 6) &= 0 \\
    \thus \lambda &= -1 \text{ or} -6.
}

For $\lambda = \lambda_1 = -1$, the eigenvector $\vec{v}_1$ is such that
\eqn{
    && (A - \lambda_1 I)\vec{v}_1 &= \vec{0} \\
    \thus \mat{-4 & 2 \\ 2 & -1}\vec{v}_1 &= \vec{0} \\
    \thus \vec{v}_1 \parallel \mat{1 \\ 2}.
}

For $\lambda = \lambda_2 = -6$, the eigenvector $\vec{v}_2$ is such that
\eqn{
    && (A - \lambda_2 I)\vec{v}_2 &= \vec{0} \\
    \thus \mat{1 & 2 \\ 2 & 4}\vec{v}_2 &= \vec{0} \\
    \thus \vec{v}_1 \parallel \mat{2 \\ -1}.
}

Thus, we may write $A$ as
\[
    A = \mat{1 & 2 \\ 2 & -1}^{-1} \mat{-1 & 0 \\ 0 & -6} \mat{1 & 2 \\ 2 & -1},
\]
and so obtain the differential equation
\[
    \frac{\diff\tilde{x}}{\dt} = \mat{-1 & 0 \\ 0 & -6} \tilde{x}.
\]
We may split up this matrix differential equation into the two decoupled scalar equations
\eqn{
    && \tilde{x}_1 &= -\tilde{x}_1 \\
    && \tilde{x}_2 &= -6\tilde{x}_2,
}
which we know have the unique solution
\eqn{
    \tilde{x}_1(t) &= \tilde{x}_1(0)e^{-t} \\
    \tilde{x}_2(t) &= \tilde{x}_2(0)e^{-6t}.
}
Now, all we have to do compute the initial conditions for $\vec{x}$, pre-multiply by $V^{-1}$ to obtain the initial conditions for $\tilde{x}$, substitute into the above to obtain the general solution for $\tilde{x}$, and pre-multiply by $V$ to finally obtain the general solution for $\vec{x}$.

Let's do so. From our circuit, we have that $V_1(0) = V_2(0) = \SI{1}{\volt}$, so $\vec{x}(0) = \mat{1 & 1}^T$. Thus, we have that
\eqn{
    && V\tilde{x}(0) &= \vec{x}(0) \\
    \thus \mat{1 & 2 \\ 2 & -1}\tilde{x}(0) &= \mat{1 \\ 1} \\
    \thus \mat{1 & 2 \\ 0 & -5} \tilde{x}(0) &= \mat{1 \\ -1} \\
    \thus \mat{1 & 2 \\ 0 & 1} \tilde{x}(0) &= \mat{1 \\ 1/5} \\
    \thus \tilde{x}(0) = \mat{1 & 0 \\ 0 & 1} \tilde{x}(0) &= \mat{3/5 \\ 1/5}.
}

Substituting these initial conditions into our solution for $\tilde{x}$, and pre-multiplying by $V$ to solve for $\vec{x}$, we obtain
\eqn{
    && \tilde{x}(t) &= \mat{\frac{3}{5}e^{-t} \\ \frac{1}{5}e^{-6t}} \\
    \thus \vec{x}(t) &= V\tilde{x}(t) \\
    &&&= \mat{1 & 2 \\ 2 & -1}\mat{\frac{3}{5}e^{-t} \\ \frac{1}{5}e^{-6t}} \\
    &&&= \mat{\frac{3}{5} e^{-t} + \frac{6}{5} e^{-6t} \\ \frac{6}{5}e^{-t} - \frac{1}{5}e^{-6t}}.
}

We have now obtained a unique solution for our state vector $\vec{t}$ as a function of time. Recall that it originally represented the voltages $V_1$ and $V_2$ across our two capacitors. Critically, we have also solved for the derivatives $\frac{\diff V_1}{\dt}$ and $\frac{\diff V_2}{\dt}$ as functions of time. Thus, we can now use Gaussian elimination to recover any of the other circuit quantities that could be interest, though we will not do so here.

\section{Inhomogenous Systems}
So far, we have developed an approach to solve for systems with no constant terms. In particular, note that we relied on writing our derivative terms $\frac{\diff V_1}{\dt}$ and $\frac{\diff V_2}{\dt}$ as linear combinations of the state variables $V_1$ and $V_2$, with no other time-dependent terms. This may not always be possible, however. For instance, recall that our original circuit was driven by an AC voltage source, which would add an additional term to our coupled differential equations, which we don't yet know how to deal with.

More formally, while we currently know how to solve equations of the form $\frac{\diff \vec{x}}{\dt} = A\vec{x}$, we do not yet know how to solve equations of the form $\frac{\diff \vec{x}}{\dt} = A\vec{x} + \vec{u}(t)$, where $\vec{u}(t)$ is some input to our system, given arbitrary initial conditions, which can be represented as an equation of the form $\vec{x}(0) = \vec{x}_0$.

To address this, we will use the principle of \emph{superposition}. Recall that, using the method of phasors, we can solve for the steady-state behavior of any circuit driven using sinusoidal voltage or current sources. In terms of our differential equations, this means that we can construct \emph{a} solution to $\frac{\diff \vec{x}}{\dt} = A\vec{x} + \vec{u}(t)$, but we may not be able to additionally impose constraints on $\vec{x}(0)$. 

Let such a solution be $\vec{x}_g(t)$, and let our desired solution be $\vec{x}(t)$. We know that
\eqn{
    && \frac{\diff \vec{x}_g(t)}{\dt} &= A\vec{x}_g(t) + \vec{u}(t) \\
    && \frac{\diff \vec{x}(t)}{\dt} &= A\vec{x}(t) + \vec{u}(t) \\
    && \vec{x}(0) &= \vec{x}_0.
}
Observe that we can rearrange these equations to obtain
\eqn{
    && \frac{\diff}{\dt} (\vec{x}(t) - \vec{x}_g(t)) &= A(\vec{x}(t) - \vec{x}_g(t)) \\
    && \vec{x}(0) - \vec{x}_g(0) &= \vec{x}_0 - \vec{x}_g(0).
}
Letting $\vec{x}_p(t) = \vec{x}(t) - \vec{x}_g(t)$, we obtain the differential equations
\eqn{
    && \frac{\diff \vec{x}_p}{\dt} &= A\vec{x}_p \\
    && \vec{x}_p(0) &= \vec{x}_0 - \vec{x}_g(0).
}
Observe that this is a homogeneous system with no inputs, so we can solve it! 

Thus, to solve an inhomogeneous set of coupled differential equations representing a circuit, we should first use phasor analysis to independently determine \emph{a} solution $\vec{x}_g(t)$, then solve the induced homogeneous system in order to obtain a \emph{transient} solution $\vec{x}_p(t)$ that accounts for the initial conditions, and put them together to obtain the final solution $\vec{x}(t) = \vec{x}_g(t) + \vec{x}_p(t)$, that takes into consideration both the initial conditions and the nonzero input / steady-state.

\section{Modes of Oscillation}
So far, we have been focusing on a singular goal - ``solving'' a circuit given initial conditions and AC inputs - but have not paid much attention to the results we've obtained along the way. In particular, observe that our solutions for each element of the state vector are of the form
\[
    \vec{x}_i = \alpha_1e^{\lambda_1 t} + \alpha_2e^{\lambda_2 t} + \ldots + \alpha_ne^{\lambda_n t}.
\]

A natural question is to ask what exactly are these $\lambda_i$? Are they real? Complex? Do they represent some sort of decay? Some sort of periodicity? As it turns out, they can be any of those, and are known most generally as the \emph{modes}\footnote{I was taught that they were called \emph{eigenmodes}, but I couldn't find that term used anywhere in EE, so \shrug.} of our system.

Notice that we should expect the real component of any $\lambda$ to be nonpositive, since otherwise it would lead to explosive growth, which we do not expect in a physical system. The real component, more generally, represents decay of some sort, since that term will go to zero over time (hence the term ``transient''). If an eigenvalue is purely imaginary, then we know from our study of phasors that it represents oscillation. Such a term can arise in our transient component, like when considering the behavior of an idealized LC-tank with no driving signal. 

We will not study these modes in great detail at this point in the course. However, one interesting aspect to observe is that these modes often arise in unexpected places. Imagine driving our initial circuit at some frequency $\omega$. A slightly tedious calculation reveals its transfer function
\[
    H(\omega) = \frac{\tilde{V_2}}{\tilde{V_{in}}} = \frac{3(j\omega + 2)}{(j\omega + 1)(j\omega + 6)}.
\]
At first glance, this transfer function doesn't look very interesting. But look at the terms in the denominator! We see roots of $j\omega = -1$ and $j\omega = -6$, which were our two eigenvalues when calculating the transient behavior of the circuit. This is no coincidence. Next lecture, we will discuss the origin of those roots, as we move beyond phasor analysis in analyzing the effects of input on coupled differential equations.

\end{document}
